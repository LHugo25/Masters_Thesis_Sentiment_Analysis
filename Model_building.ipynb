{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!pip install xgboost\n",
    "!pip install en_core_web_md-3.7.1-py3-none-any.whl\n",
    "!pip install imblearn\n",
    "!pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform imports and load dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import spacy\n",
    "import re # For regular expressions|||\n",
    "import string # For handling string\n",
    "import nltk\n",
    "import pickle\n",
    "import sklearn\n",
    "import vaderSentiment\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.util import ngrams\n",
    "#import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from collections import  Counter\n",
    "#import gensim\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "#from textblob import TextBlob\n",
    "#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from scipy.sparse import csr_matrix\n",
    "#from imblearn import under_sampling, over_sampling\n",
    "#from imblearn.datasets import make_imbalance\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "data = pd.read_csv(\"LabData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing:\n",
    "df1 = data\n",
    "#CLEANING DATA\n",
    "#drop duplicates\n",
    "df1 = df1.drop_duplicates(keep='first')\n",
    "#removing numbers\n",
    "df1['text'] = df1['text'].astype('string')\n",
    "df1.dropna(subset=['text'], inplace=True)\n",
    "#lowercase\n",
    "df1['text']=df1['text'].apply(lambda x: x.lower())\n",
    "#removing mentions\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub(\"@[A-Za-z0-9_]+\",'', x))\n",
    "#removing numbers and words containing numbers\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n",
    "#removing hashtags?\n",
    "#df1['text']=df1['text'].apply(lambda x: re.sub(\"#[A-Za-z0-9_]+\",'', x))\n",
    "#df1['text']=df1['text'].apply(lambda x: re.split('#|_', x))\n",
    "df1['text']=df1['text'].apply(lambda x: \" \".join(word.strip() for word in re.split('#|_', x)))\n",
    "#removing links\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub(r\"http\\S+\",'', x))\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub(rwww.\\S+,'', x))\n",
    "#removing punctuation\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n",
    "#removing empty spaces and special characters\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub('[^A-Za-z0-9]+',' ',x))\n",
    "#stopwords\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "# Print the set of spaCy's default stop words (remember that sets are unordered):\n",
    "print(nlp.Defaults.stop_words)\n",
    "#add twitter as a stop word and rt\n",
    "# Add the word to the set of stop words. Use LOWERCASE!\n",
    "nlp.Defaults.stop_words.add('twitter')\n",
    "nlp.Defaults.stop_words.add('rt')\n",
    "nlp.Defaults.stop_words.add('s')\n",
    "nlp.Defaults.stop_words.add('tweet')\n",
    "nlp.Defaults.stop_words.add('trend')\n",
    "nlp.Defaults.stop_words.add('follower')\n",
    "nlp.Defaults.stop_words.add('null')\n",
    "nlp.Defaults.stop_words.add('http')\n",
    "nlp.Defaults.stop_words.add('url')\n",
    "nlp.Defaults.stop_words.add('trend')\n",
    "nlp.Defaults.stop_words.add('trending')\n",
    "# Set the stop_word tag on the lexeme\n",
    "nlp.vocab['twitter'].is_stop = True\n",
    "nlp.vocab['rt'].is_stop = True\n",
    "nlp.vocab['s'].is_stop = True\n",
    "nlp.vocab['tweet'].is_stop = True\n",
    "nlp.vocab['trend'].is_stop = True\n",
    "nlp.vocab['follower'].is_stop = True\n",
    "nlp.vocab['null'].is_stop = True\n",
    "nlp.vocab['http'].is_stop = True\n",
    "nlp.vocab['url'].is_stop = True\n",
    "nlp.vocab['trend'].is_stop = True\n",
    "nlp.vocab['trending'].is_stop = True\n",
    "stop = nlp.Defaults.stop_words\n",
    "#lemmatisation\n",
    "# Loading model\n",
    "#nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization with stopwords removal\n",
    "#df1[\"text\"]=df1[\"text\"].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if ((token.is_stop==False)&((token.pos_==\"ADJ\")|(token.pos_==\"NOUN\")|(token.pos_==\"VERB\")))]))\n",
    "#df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"text\"]=df1[\"text\"].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if ((token.is_stop==False))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of empty text rows\n",
    "df1 = df1.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DF and TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF if necessary depending on feature selection\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Create TF vectorizer\n",
    "#vectorizer = CountVectorizer)\n",
    "#DF = vectorizer.fit_transform(data['text'])\n",
    "#DF = pd.DataFrame(DF)\n",
    "#DF = pd.DataFrame(data=DF, columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Fit and transform the training data\n",
    "X_tfidf = vectorizer.fit_transform(data['text'])\n",
    "X_train_tfidf = X_tfidf.toarray()\n",
    "#checking type\n",
    "#test.dtypes()\n",
    "DF = pd.DataFrame(data=X_train_tfidf, columns=vectorizer.get_feature_names_out())\n",
    "DF.head\n",
    "#X_tfidf = X_tfidf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames\n",
    "DF['text'] = data['text']\n",
    "DF['label'] = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedDF = DF.sample(frac=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train set\n",
    "#divide into training, validation and test data (DF)\n",
    "from sklearn.model_selection import train_test_split\n",
    "ind = reducedDF.index\n",
    "#sample the indices and then drop from current data set\n",
    "# Assuming you have features X and labels y\n",
    "df_train, df_test= train_test_split(ind, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now create the data sets\n",
    "TrainSet = reducedDF.drop(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del reducedDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look at the Train set\n",
    "TrainSet.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look at the label column and its unique values\n",
    "TrainSet['label'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix the label column. Ensure labels take on only the values [0,1,-1]\n",
    "mapping = {'-2': -1, '-1': -1, '0': 0, '1': 1, '9': 0, 'FALSE': 0, 'TRUE': 0, 'lab': 0}\n",
    "TrainSet['label'] = TrainSet['label'].map(mapping)\n",
    "#TrainSet.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that the mapping worked\n",
    "TrainSet['label'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TrainSet.dropna(subset= \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del TrainSet['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test set\n",
    "#training set\n",
    "y_train = TrainSet[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del TrainSet['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ValSet = pd.read_csv(\"ReducedData.csv\")\n",
    "ValSet = pd.read_csv(\"TestReducedtfidf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the same for Val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Letting the label column be values [0,-1,1]\n",
    "mapping = {'-2': -1, '-1': -1, '0': 0, '1': 1, '9': 0, 'FALSE': 0, 'TRUE': 0, 'lab': 0}\n",
    "ValSet['label'] = ValSet['label'].map(mapping)\n",
    "ValSet['label']  = ValSet['label'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValSet[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = ValSet[\"label\"]\n",
    "y_val.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete label and text columns \n",
    "del ValSet['Unnamed: 0']\n",
    "del ValSet['Unnamed: 0.1']\n",
    "del ValSet['text']\n",
    "del ValSet['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValSet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "# grid search\n",
    "#nfeatures = [1000]\n",
    "#nfeatures = [2000]\n",
    "nfeatures = [1000] #4000, 5000, 6000, 8000, 10000]\n",
    "rfn = [50]\n",
    "max_values = [10]\n",
    " \n",
    "# setting the sets\n",
    "#Xtrain = TrainSet\n",
    "#XVal = ValSet\n",
    "#ytrain = yX[0:1000]\n",
    "#ytrain = y_train\n",
    "#yval = yV[0:1000]\n",
    "#yval = y_val\n",
    " \n",
    "# setting objects to store performance results\n",
    "results = []\n",
    " \n",
    "for nf in nfeatures:\n",
    "    for n in rfn:\n",
    "        for m in max_values:\n",
    "            # Random forest classifier\n",
    "            # Feature selection\n",
    "            chi2_features = SelectKBest(score_func=chi2, k=nf)\n",
    "            ChiX_selected = chi2_features.fit_transform(TrainSet, y_train)\n",
    "            support = chi2_features.get_support()\n",
    " \n",
    "            # Get the selected feature names using the support indices\n",
    "            selected_feature_names = TrainSet.columns[support]\n",
    "            X_selected = TrainSet[selected_feature_names]\n",
    " \n",
    "            # Random Oversampling\n",
    "            ros = RandomOverSampler(random_state=0)\n",
    "            X_oversampled1, y_oversampled1 = ros.fit_resample(X_selected, y_train)\n",
    " \n",
    "            # Fit the Random Forest model to the training data\n",
    "            RFC1 = RandomForestClassifier(n_estimators=n, max_depth=m)\n",
    "            RFC1.fit(X_oversampled1, y_oversampled1)\n",
    " \n",
    "            # equate the column variables for train and validation (since we've done a FS)\n",
    "            test = pd.DataFrame(TestSet)\n",
    "            test_cols = TestSet.columns\n",
    "            common_cols = selected_feature_names.intersection(test_cols)\n",
    "            testnew = TestSet[common_cols]\n",
    " \n",
    "            # Predict the labels of the testing data\n",
    "            y_pred1 = RFC1.predict(testnew)\n",
    " \n",
    "            # Print the accuracy of the model\n",
    "            accuracy = accuracy_score(y_val, y_pred1)\n",
    " \n",
    "            # Print the MSE of the model\n",
    "            conf_matrix =  confusion_matrix(y_val, y_pred1)\n",
    "            mcr = (conf_matrix[0, 1] + conf_matrix[1, 0]) / len(y_val)\n",
    " \n",
    "            # keep record of the parameters and results\n",
    "            results.append({\n",
    "                'nfeatures': nf,\n",
    "                'n_estimators': n,\n",
    "                'ax_depth': m,\n",
    "                'accuracy': accuracy,\n",
    "                'mcr': mcr\n",
    "            })\n",
    "\n",
    "# print the results\n",
    "for result in results:\n",
    "    print(result)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results\n",
    "results = pd.DataFrame(results)\n",
    "results.to_excel(\"RFResults2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import test set\n",
    "import pandas as pd\n",
    "TestSet = pd.read_csv(\"TestReducedtfidf.csv\")\n",
    "TestSet.shape\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'-2': -1, '-1': -1, '0': 0, '1': 1, '9': 0, 'FALSE': 0, 'TRUE': 0, 'lab': 0}\n",
    "TestSet['label'] = TestSet['label'].map(mapping)\n",
    "TestSet['label']  = TestSet['label'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestSet[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = TestSet[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del TestSet['Unnamed: 0']\n",
    "#del TestSet['Unnamed: 0.1']\n",
    "del TestSet['text']\n",
    "del TestSet['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "# Random forest classifier\n",
    "# Feature selection\n",
    "chi2_features = SelectKBest(score_func=chi2, k=1000)\n",
    "ChiX_selected = chi2_features.fit_transform(TrainSet, y_train)\n",
    "support = chi2_features.get_support()\n",
    " \n",
    "# Get the selected feature names using the support indices\n",
    "selected_feature_names = TrainSet.columns[support]\n",
    "X_selected = TrainSet[selected_feature_names]\n",
    " \n",
    "# Random Oversampling\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_oversampled1, y_oversampled1 = ros.fit_resample(X_selected, y_train)\n",
    " \n",
    "# Fit the Random Forest model to the training data\n",
    "RFC1 = RandomForestClassifier(n_estimators=50, max_depth=10)\n",
    "RFC1.fit(X_oversampled1, y_oversampled1)\n",
    " \n",
    "# equate the column variables for train and validation (since we've done a FS)\n",
    "test = pd.DataFrame(TestSet)\n",
    "test_cols = TestSet.columns\n",
    "common_cols = selected_feature_names.intersection(test_cols)\n",
    "testnew = TestSet[common_cols]\n",
    " \n",
    "# Predict the labels of the testing data\n",
    "y_pred1 = RFC1.predict(testnew)\n",
    " \n",
    "# Print the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred1)\n",
    " \n",
    "# Calculate MCR for three-class problem\n",
    "conf_matrix =  confusion_matrix(y_test, y_pred1)\n",
    "mcr = (conf_matrix[0, 1] + conf_matrix[0, 2] + conf_matrix[1, 0] + conf_matrix[1, 2] + conf_matrix[2, 0] + conf_matrix[2, 1]) / len(y_test)\n",
    " \n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_test, y_pred1, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "# grid search\n",
    "nfeatures = [2000]#, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "kernel = ['poly', 'rbf', 'sigmoid']\n",
    "C = [50, 10]#, 1.0] #, 0.1, 0.01]\n",
    " \n",
    "# setting the sets\n",
    "#Xtrain = TrainSet[0:100]\n",
    "#XVal = ValSet[0:100]\n",
    "#ytrain = yX[0:1000]\n",
    "#ytrain = y_train[0:100]\n",
    "#yval = yV[0:1000]\n",
    "#yval = y_val[0:100]\n",
    " \n",
    "# setting objects to store performance results\n",
    "results2 = []\n",
    " \n",
    "for nf in nfeatures:\n",
    "    for k in kernel:\n",
    "        for c in C:\n",
    "            # Random forest classifier\n",
    "            # Feature selection\n",
    "            svd = TruncatedSVD(n_components=nf)\n",
    "            X_svd = svd.fit_transform(TrainSet)\n",
    " \n",
    "            # Random Oversampling\n",
    "            ros = RandomOverSampler(random_state=0)\n",
    "            X_oversampled1, y_oversampled1 = ros.fit_resample(X_svd, y_train)\n",
    " \n",
    "            # Fit the Random Forest model to the training data\n",
    "            SVC1 = OneVsRestClassifier(SVC(kernel = k, C=c, gamma=\"auto\"))\n",
    "            SVC1.fit(X_oversampled1, y_oversampled1)\n",
    " \n",
    "            # Predict the labels of the testing data\n",
    "           \n",
    "            testnew = svd.transform(ValSet)\n",
    "            y_pred1 = SVC1.predict(testnew)\n",
    " \n",
    "            # Print the accuracy of the model\n",
    "            accuracy = accuracy_score(y_val, y_pred1)\n",
    " \n",
    "            # Print the MSE of the model\n",
    "            conf_matrix =  confusion_matrix(y_val, y_pred1)\n",
    "            mcr = (conf_matrix[0, 1] + conf_matrix[0, 2] + conf_matrix[1, 0] + conf_matrix[1, 2] + conf_matrix[2, 0] + conf_matrix[2, 1]) / len(y_val)\n",
    " \n",
    "            # keep record of the parameters and results\n",
    "            results2.append({\n",
    "                'nfeatures': nf,\n",
    "                'kernel':k,\n",
    "                'C': c,\n",
    "                'accuracy': accuracy,\n",
    "                'mcr': mcr\n",
    "            })\n",
    " \n",
    "# print the results\n",
    "for result in results2:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results\n",
    "results = pd.DataFrame(results2)\n",
    "results.to_excel(\"SVCResults.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "# grid search\n",
    "nfeatures = [1000]#, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "kernel = ['poly', 'rbf', 'sigmoid']\n",
    "#C = [50, 10, 1.0,\n",
    "C= [0.1, 0.01]\n",
    " \n",
    "# setting the sets\n",
    "#Xtrain = TrainSet[0:100]\n",
    "#XVal = ValSet[0:100]\n",
    "#ytrain = yX[0:1000]\n",
    "#ytrain = y_train[0:100]\n",
    "#yval = yV[0:1000]\n",
    "#yval = y_val[0:100]\n",
    " \n",
    "# setting objects to store performance results\n",
    "results2 = []\n",
    " \n",
    "for nf in nfeatures:\n",
    "    for k in kernel:\n",
    "        for c in C:\n",
    "            # Random forest classifier\n",
    "            # Feature selection\n",
    "            svd = TruncatedSVD(n_components=nf)\n",
    "            X_svd = svd.fit_transform(TrainSet)\n",
    " \n",
    "            # Random Oversampling\n",
    "            ros = RandomOverSampler(random_state=0)\n",
    "            X_oversampled1, y_oversampled1 = ros.fit_resample(X_svd, y_train)\n",
    " \n",
    "            # Fit the Random Forest model to the training data\n",
    "            SVC1 = OneVsRestClassifier(SVC(kernel = k, C=c, gamma=\"auto\"))\n",
    "            SVC1.fit(X_oversampled1, y_oversampled1)\n",
    " \n",
    "            # Predict the labels of the testing data\n",
    "            testnew = svd.transform(ValSet)\n",
    "            y_pred1 = SVC1.predict(testnew)\n",
    " \n",
    "            # Print the accuracy of the model\n",
    "            accuracy = accuracy_score(y_val, y_pred1)\n",
    " \n",
    "            # Print the MSE of the model\n",
    "            conf_matrix =  confusion_matrix(y_val, y_pred1)\n",
    "            mcr = (conf_matrix[0, 1] + conf_matrix[1, 0]) / len(y_val)\n",
    " \n",
    "            # keep record of the parameters and results\n",
    "            results2.append({\n",
    "                'nfeatures': nf,\n",
    "                'kernel':k,\n",
    "                'C': c,\n",
    "                'accuracy': accuracy,\n",
    "                'mcr': mcr\n",
    "            })\n",
    " \n",
    "# print the results\n",
    "for result in results2:\n",
    "    print(result)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results\n",
    "results = pd.DataFrame(results2)\n",
    "results.to_excel(\"SVCResults3.xlsx\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_val_enc = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label = 0:negative, 1:neutral, 2:positive\n",
    "# libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    " \n",
    "# grid search\n",
    "nfeatures = [5000] #, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "maxdepth = [None,5, 10]\n",
    "n_estimators= [200]#50]#,100,200]\n",
    "learning_rate = [0.01]#0.001]#, 0.01, 0.1]\n",
    "subsample = [0.5, 0.7, 1.0]\n",
    " \n",
    "# setting objects to store performance results\n",
    "results3 = []\n",
    " \n",
    "for nf in nfeatures:\n",
    "    for m in maxdepth:\n",
    "        for l in learning_rate:\n",
    "            for ss in subsample:\n",
    "                for n in n_estimators:\n",
    "                    # XGboost\n",
    "                     # Feature selection\n",
    "                    svd = TruncatedSVD(n_components=nf)\n",
    "                    X_svd = svd.fit_transform(TrainSet)\n",
    " \n",
    "                    # Random Oversampling\n",
    "                    ros = RandomOverSampler(random_state=0)\n",
    "                    X_oversampled1, y_oversampled1 = ros.fit_resample(X_svd, y_train_enc)\n",
    "                    del X_svd\n",
    " \n",
    "                    # Fit the Random Forest model to the training data\n",
    "                    XGB1 = XGBClassifier(max_depth = m, learning_rate=l, n_estimators = n, subsample=ss)\n",
    "                    XGB1.fit(X_oversampled1, y_oversampled1)\n",
    "                    del X_oversampled1\n",
    "                    del y_oversampled1\n",
    " \n",
    "                    # Predict the labels of the testing data\n",
    "                    testnew = svd.transform(ValSet)\n",
    "                    y_pred1 = XGB1.predict(testnew)\n",
    " \n",
    "                    # Print the accuracy of the model\n",
    "                    accuracy = accuracy_score(y_val_enc, y_pred1)\n",
    " \n",
    "                    #maybe add a more sensitive performance metric\n",
    " \n",
    "                    # Print the MSE of the model\n",
    "                    conf_matrix =  confusion_matrix(y_val_enc, y_pred1)\n",
    "                    mcr = (conf_matrix[0, 1] + conf_matrix[0, 2] + conf_matrix[1, 0] + conf_matrix[1, 2] + conf_matrix[2, 0] + conf_matrix[2, 1]) / len(y_val_enc)\n",
    " \n",
    "                    del XGB1\n",
    "                    del y_pred1\n",
    "                    del conf_matrix\n",
    "                    print(\"step 4 done\")\n",
    "                    # keep record of the parameters and results\n",
    "                    results3.append({\n",
    "                        'nfeatures': nf,\n",
    "                        'learning_rate': l,\n",
    "                        'max_depth': m,\n",
    "                        'subsample': ss,\n",
    "                        'accuracy': accuracy,\n",
    "                        'mcr': mcr\n",
    "                    })\n",
    " \n",
    "# print the results\n",
    "for result in results3:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results\n",
    "results = pd.DataFrame(results3)\n",
    "results.to_excel(\"XGResults3.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run selected models on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import test set\n",
    "import pandas as pd\n",
    "TestSet = pd.read_csv(\"TestReducedtfidf.csv\")\n",
    "TestSet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean respone variable\n",
    "mapping = {'-2': -1, '-1': -1, '0': 0, '1': 1, '9': 0, 'FALSE': 0, 'TRUE': 0, 'lab': 0}\n",
    "TestSet['label'] = TestSet['label'].map(mapping)\n",
    "TestSet['label']  = TestSet['label'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestSet[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = TestSet[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del TestSet['Unnamed: 0']\n",
    "#del TestSet['Unnamed: 0.1']\n",
    "del TestSet['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del TestSet['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestSet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TestSet = TestSet.fillna(0)\n",
    "TestSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test on Random Forest on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "# Random forest classifier\n",
    "# Feature selection\n",
    "chi2_features = SelectKBest(score_func=chi2, k=1000)\n",
    "ChiX_selected = chi2_features.fit_transform(TrainSet, y_train)\n",
    "support = chi2_features.get_support()\n",
    " \n",
    "# Get the selected feature names using the support indices\n",
    "selected_feature_names = TrainSet.columns[support]\n",
    "X_selected = TrainSet[selected_feature_names]\n",
    " \n",
    "# Random Oversampling\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_oversampled1, y_oversampled1 = ros.fit_resample(X_selected, y_train)\n",
    " \n",
    "# Fit the Random Forest model to the training data\n",
    "RFC1 = RandomForestClassifier(n_estimators=50, max_depth=10)\n",
    "RFC1.fit(X_oversampled1, y_oversampled1)\n",
    " \n",
    "# equate the column variables for train and validation (since we've done a FS)\n",
    "test = pd.DataFrame(TestSet)\n",
    "test_cols = TestSet.columns\n",
    "common_cols = selected_feature_names.intersection(test_cols)\n",
    "testnew = TestSet[common_cols]\n",
    " \n",
    "# Predict the labels of the testing data\n",
    "y_pred1 = RFC1.predict(testnew)\n",
    " \n",
    "# Print the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred1)\n",
    " \n",
    "# Calculate MCR for three-class problem\n",
    "conf_matrix =  confusion_matrix(y_test, y_pred1)\n",
    "mcr = (conf_matrix[0, 1] + conf_matrix[0, 2] + conf_matrix[1, 0] + conf_matrix[1, 2] + conf_matrix[2, 0] + conf_matrix[2, 1]) / len(y_test)\n",
    " \n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_test, y_pred1, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test on SVM on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run SVC on test set\n",
    " \n",
    "# libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "# Random forest classifier\n",
    "# Feature selection\n",
    "svd = TruncatedSVD(n_components=750)\n",
    "X_svd = svd.fit_transform(TrainSet)\n",
    " \n",
    "# Random Oversampling\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_oversampled1, y_oversampled1 = ros.fit_resample(X_svd, y_train)\n",
    " \n",
    "# Fit the Random Forest model to the training data\n",
    "SVC1 = OneVsRestClassifier(SVC(kernel = \"poly\", C=0.1, gamma=\"auto\"))\n",
    "SVC1.fit(X_oversampled1, y_oversampled1)\n",
    " \n",
    "# Predict the labels of the testing data\n",
    "testnew = svd.transform(TestSet)\n",
    "y_pred1 = SVC1.predict(testnew)\n",
    " \n",
    "# Print the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred1)\n",
    " \n",
    "# Print the MSE of the model\n",
    "conf_matrix =  confusion_matrix(y_test, y_pred1)\n",
    " \n",
    "# Calculate MCR for three-class problem\n",
    "mcr = (conf_matrix[0, 1] + conf_matrix[0, 2] + conf_matrix[1, 0] + conf_matrix[1, 2] + conf_matrix[2, 0] + conf_matrix[2, 1]) / len(y_test)\n",
    " \n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_test, y_pred1, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing XGBoost on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_val_enc = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValSet = ValSet.drop(columns='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label = 0:negative, 1:neutral, 2:positive\n",
    "# libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    " \n",
    "nf = 500\n",
    "# Feature selection\n",
    "svd = TruncatedSVD(n_components=nf)\n",
    "X_svd = svd.fit_transform(TrainSet)\n",
    " \n",
    "# Random Oversampling\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_oversampled1, y_oversampled1 = ros.fit_resample(X_svd, y_train_enc)\n",
    "del X_svd\n",
    " \n",
    "# Fit the Random Forest model to the training data\n",
    "XGB1 = XGBClassifier(max_depth = 10, learning_rate=0.1, n_estimators = 200, subsample=0.5)\n",
    "XGB1.fit(X_oversampled1, y_oversampled1)\n",
    "del X_oversampled1\n",
    "del y_oversampled1\n",
    " \n",
    "# Predict the labels of the testing data\n",
    "testnew = svd.transform(ValSet)\n",
    "y_pred1 = XGB1.predict(testnew)\n",
    " \n",
    "# Print the accuracy of the model\n",
    "accuracy = accuracy_score(y_val_enc, y_pred1)\n",
    " \n",
    "#maybe add a more sensitive performance metric\n",
    " \n",
    "# Print the MSE of the model\n",
    "conf_matrix =  confusion_matrix(y_val_enc, y_pred1)\n",
    "mcr = (conf_matrix[0, 1] + conf_matrix[0, 2] + conf_matrix[1, 0] + conf_matrix[1, 2] + conf_matrix[2, 0] + conf_matrix[2, 1]) / len(y_val_enc)\n",
    " \n",
    " \n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_val_enc, y_pred1, average=\"weighted\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
