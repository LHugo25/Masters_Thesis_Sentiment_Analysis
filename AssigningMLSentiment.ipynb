{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install packages\n",
    "!pip install spacy\n",
    "!pip install xgboost\n",
    "!pip install en_core_web_md-3.7.1-py3-none-any.whl\n",
    "!pip install imblearn\n",
    "!pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform imports and load dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import spacy\n",
    "import re # For regular expressions|||\n",
    "import string # For handling string\n",
    "import nltk\n",
    "import pickle\n",
    "import sklearn\n",
    "import vaderSentiment\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.util import ngrams\n",
    "#import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from collections import  Counter\n",
    "#import gensim\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "#from textblob import TextBlob\n",
    "#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from scipy.sparse import csr_matrix\n",
    "#from imblearn import under_sampling, over_sampling\n",
    "#from imblearn.datasets import make_imbalance\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full labeled data set \n",
    "data = pd.read_csv(\"LabData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing:\n",
    "df1 = data\n",
    "#CLEANING DATA\n",
    "#drop duplicates\n",
    "df1 = df1.drop_duplicates(keep='first')\n",
    "#removing numbers\n",
    "df1['text'] = df1['text'].astype('string')\n",
    "df1.dropna(subset=['text'], inplace=True)\n",
    "#lowercase\n",
    "df1['text']=df1['text'].apply(lambda x: x.lower())\n",
    "#removing mentions\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub(\"@[A-Za-z0-9_]+\",'', x))\n",
    "#removing numbers and words containing numbers\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n",
    "#removing hashtags?\n",
    "#df1['text']=df1['text'].apply(lambda x: re.sub(\"#[A-Za-z0-9_]+\",'', x))\n",
    "#df1['text']=df1['text'].apply(lambda x: re.split('#|_', x))\n",
    "df1['text']=df1['text'].apply(lambda x: \" \".join(word.strip() for word in re.split('#|_', x)))\n",
    "#removing links\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub(r\"http\\S+\",'', x))\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub(rwww.\\S+,'', x))\n",
    "#removing punctuation\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n",
    "#removing empty spaces and special characters\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub('[^A-Za-z0-9]+',' ',x))\n",
    "#stopwords\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "# Print the set of spaCy's default stop words (remember that sets are unordered):\n",
    "print(nlp.Defaults.stop_words)\n",
    "#add twitter as a stop word and rt\n",
    "# Add the word to the set of stop words. Use LOWERCASE!\n",
    "nlp.Defaults.stop_words.add('twitter')\n",
    "nlp.Defaults.stop_words.add('rt')\n",
    "nlp.Defaults.stop_words.add('s')\n",
    "nlp.Defaults.stop_words.add('tweet')\n",
    "nlp.Defaults.stop_words.add('trend')\n",
    "nlp.Defaults.stop_words.add('follower')\n",
    "nlp.Defaults.stop_words.add('null')\n",
    "nlp.Defaults.stop_words.add('http')\n",
    "nlp.Defaults.stop_words.add('url')\n",
    "nlp.Defaults.stop_words.add('trend')\n",
    "nlp.Defaults.stop_words.add('trending')\n",
    "# Set the stop_word tag on the lexeme\n",
    "nlp.vocab['twitter'].is_stop = True\n",
    "nlp.vocab['rt'].is_stop = True\n",
    "nlp.vocab['s'].is_stop = True\n",
    "nlp.vocab['tweet'].is_stop = True\n",
    "nlp.vocab['trend'].is_stop = True\n",
    "nlp.vocab['follower'].is_stop = True\n",
    "nlp.vocab['null'].is_stop = True\n",
    "nlp.vocab['http'].is_stop = True\n",
    "nlp.vocab['url'].is_stop = True\n",
    "nlp.vocab['trend'].is_stop = True\n",
    "nlp.vocab['trending'].is_stop = True\n",
    "stop = nlp.Defaults.stop_words\n",
    "#lemmatisation\n",
    "# Loading model\n",
    "#nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization with stopwords removal\n",
    "#df1[\"text\"]=df1[\"text\"].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if ((token.is_stop==False)&((token.pos_==\"ADJ\")|(token.pos_==\"NOUN\")|(token.pos_==\"VERB\")))]))\n",
    "#df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"text\"]=df1[\"text\"].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if ((token.is_stop==False))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of empty text rows\n",
    "df1 = df1.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Fit and transform the training data\n",
    "X_tfidf = vectorizer.fit_transform(data['text'])\n",
    "X_train_tfidf = X_tfidf.toarray()\n",
    "#checking type\n",
    "#test.dtypes()\n",
    "DF = pd.DataFrame(data=X_train_tfidf, columns=vectorizer.get_feature_names_out())\n",
    "DF.head\n",
    "#X_tfidf = X_tfidf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames\n",
    "DF['text'] = data['text']\n",
    "DF['label'] = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedDF = DF.sample(frac=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train set\n",
    "#divide into training, validation and test data (DF)\n",
    "from sklearn.model_selection import train_test_split\n",
    "ind = reducedDF.index\n",
    "#sample the indices and then drop from current data set\n",
    "# Assuming you have features X and labels y\n",
    "df_train, df_test= train_test_split(ind, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now create the data sets\n",
    "TrainSet = reducedDF.drop(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del reducedDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the response variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TrainSet.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'-2': -1, '-1': -1, '0': 0, '1': 1, '9': 0, 'FALSE': 0, 'TRUE': 0, 'lab': 0}\n",
    "TrainSet['label'] = TrainSet['label'].map(mapping)\n",
    "#TrainSet.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training set\n",
    "#TrainSet = ResponseClean(TrainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet['label'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del TrainSet['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test set\n",
    "#training set\n",
    "y_train = TrainSet[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del TrainSet['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "# Random forest classifier\n",
    "# Feature selection\n",
    "chi2_features = SelectKBest(score_func=chi2, k=1000)\n",
    "ChiX_selected = chi2_features.fit_transform(TrainSet, y_train)\n",
    "support = chi2_features.get_support()\n",
    " \n",
    "# Get the selected feature names using the support indices\n",
    "selected_feature_names = TrainSet.columns[support]\n",
    "X_selected = TrainSet[selected_feature_names]\n",
    " \n",
    "# Random Oversampling\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_oversampled1, y_oversampled1 = ros.fit_resample(X_selected, y_train)\n",
    " \n",
    "# Fit the Random Forest model to the training data\n",
    "RFC1 = RandomForestClassifier(n_estimators=50, max_depth=10)\n",
    "RFC1.fit(X_oversampled1, y_oversampled1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need to get the output per data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OGdata = pd.read_csv(\"ConsFinal.csv\")\n",
    "#due to size had to split it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OGdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide the data\n",
    "del data1\n",
    "data1 = OGdata[800000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data (1,2,3,4,5)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing:\n",
    "df1 = data1\n",
    "#CLEANING DATA\n",
    "#drop duplicates\n",
    "df1 = df1.drop_duplicates(keep='first')\n",
    "#removing numbers\n",
    "df1['text'] = df1['text'].astype('string')\n",
    "df1.dropna(subset=['text'], inplace=True)\n",
    "#lowercase\n",
    "df1['text']=df1['text'].apply(lambda x: x.lower())\n",
    "#removing mentions\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub(\"@[A-Za-z0-9_]+\",'', x))\n",
    "#removing numbers and words containing numbers\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n",
    "#removing hashtags?\n",
    "#df1['text']=df1['text'].apply(lambda x: re.sub(\"#[A-Za-z0-9_]+\",'', x))\n",
    "#df1['text']=df1['text'].apply(lambda x: re.split('#|_', x))\n",
    "df1['text']=df1['text'].apply(lambda x: \" \".join(word.strip() for word in re.split('#|_', x)))\n",
    "#removing links\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub(r\"http\\S+\",'', x))\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub(rwww.\\S+,'', x))\n",
    "#removing punctuation\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n",
    "#removing empty spaces and special characters\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub('[^A-Za-z0-9]+',' ',x))\n",
    "#stopwords\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "# Print the set of spaCy's default stop words (remember that sets are unordered):\n",
    "print(nlp.Defaults.stop_words)\n",
    "#add twitter as a stop word and rt\n",
    "# Add the word to the set of stop words. Use LOWERCASE!\n",
    "nlp.Defaults.stop_words.add('twitter')\n",
    "nlp.Defaults.stop_words.add('rt')\n",
    "nlp.Defaults.stop_words.add('s')\n",
    "nlp.Defaults.stop_words.add('tweet')\n",
    "nlp.Defaults.stop_words.add('trend')\n",
    "nlp.Defaults.stop_words.add('follower')\n",
    "nlp.Defaults.stop_words.add('null')\n",
    "nlp.Defaults.stop_words.add('http')\n",
    "nlp.Defaults.stop_words.add('url')\n",
    "nlp.Defaults.stop_words.add('trend')\n",
    "nlp.Defaults.stop_words.add('trending')\n",
    "# Set the stop_word tag on the lexeme\n",
    "nlp.vocab['twitter'].is_stop = True\n",
    "nlp.vocab['rt'].is_stop = True\n",
    "nlp.vocab['s'].is_stop = True\n",
    "nlp.vocab['tweet'].is_stop = True\n",
    "nlp.vocab['trend'].is_stop = True\n",
    "nlp.vocab['follower'].is_stop = True\n",
    "nlp.vocab['null'].is_stop = True\n",
    "nlp.vocab['http'].is_stop = True\n",
    "nlp.vocab['url'].is_stop = True\n",
    "nlp.vocab['trend'].is_stop = True\n",
    "nlp.vocab['trending'].is_stop = True\n",
    "stop = nlp.Defaults.stop_words\n",
    "#lemmatisation\n",
    "# Loading model\n",
    "#nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization with stopwords removal\n",
    "#df1[\"text\"]=df1[\"text\"].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if ((token.is_stop==False)&((token.pos_==\"ADJ\")|(token.pos_==\"NOUN\")|(token.pos_==\"VERB\")))]))\n",
    "#df1.head()\n",
    "df1[\"text\"]=df1[\"text\"].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if ((token.is_stop==False))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of empty text rows\n",
    "df1 = df1.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Fit and transform the training data\n",
    "X_tfidf = vectorizer.fit_transform(data['text'])\n",
    "X_train_tfidf = X_tfidf.toarray()\n",
    "#checking type\n",
    "#test.dtypes()\n",
    "DF = pd.DataFrame(data=X_train_tfidf, columns=vectorizer.get_feature_names_out())\n",
    "DF.head\n",
    "#X_tfidf = X_tfidf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equate the column variables\n",
    "test = pd.DataFrame(DF)\n",
    "test_cols = test.columns\n",
    "common_cols = selected_feature_names.intersection(test_cols)\n",
    "testnew = test[common_cols]\n",
    "testnew = testnew.reindex(columns=selected_feature_names, fill_value=0)\n",
    "del y_pred1\n",
    "# Predict the labels of the testing data\n",
    "y_pred1 = RFC1.predict(testnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred1) #sense check\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y_values'] = y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = data\n",
    "RF.to_csv(\"RF9.csv\")\n",
    "del DF\n",
    "del X_tfidf"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
