{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\NB332021\\\\OneDrive - Nedbank\\\\Desktop\\\\Thesis\\\\Data\\\\ConsFinal.csv\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the data formatting\n",
    "data[\"created_at\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape of data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the first row\n",
    "data.drop(index=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now convert the data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_non_date_values(df, column_name):\n",
    "    for i in df.index:\n",
    "        try:\n",
    "            df.loc[i, column_name] = pd.to_datetime(df.loc[i, column_name])\n",
    "        except ValueError:\n",
    "            if i > 0:\n",
    "                df.loc[i, column_name] = df.loc[i-1, column_name]\n",
    "            else:\n",
    "                df.loc[i, column_name] = df.loc[i+1, column_name]\n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_non_date_values(data, \"created_at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    " \n",
    "# assume 'df' is your DataFrame and 'column_name' is the column you want to format\n",
    "data['created_at'] = pd.to_datetime(data['created_at']).dt.strftime('%b-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"C:\\\\Users\\\\Desktop\\\\Thesis\\\\Data\\\\datesCorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment package: VaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prep data for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform imports and load dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import spacy\n",
    "import re # For regular expressions|||\n",
    "import string # For handling string\n",
    "import nltk\n",
    "import vaderSentiment\n",
    "#import matplotlib.pyplot as plt\n",
    "from nltk.util import ngrams\n",
    "#import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from collections import  Counter\n",
    " \n",
    "#data preprocessing:\n",
    "df1 = data.copy()\n",
    "#CLEANING DATA\n",
    "#drop duplicates\n",
    "data = data.drop_duplicates(keep='first')\n",
    "#removing numbers\n",
    "df1['text'] = df1['text'].astype('string')\n",
    "df1.dropna(subset=['text'], inplace=True)\n",
    "#lowercase\n",
    "df1['text']=df1['text'].apply(lambda x: x.lower())\n",
    "#removing mentions\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub(\"@[A-Za-z0-9_]+\",'', x))\n",
    "#removing numbers and words containing numbers\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n",
    "#removing hashtags?\n",
    "#df1['text']=df1['text'].apply(lambda x: re.sub(\"#[A-Za-z0-9_]+\",'', x))\n",
    "#df1['text']=df1['text'].apply(lambda x: re.split('#|_', x))\n",
    "df1['text']=df1['text'].apply(lambda x: \" \".join(word.strip() for word in re.split('#|_', x)))\n",
    "#removing links\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub(r\"http\\S+\",'', x))\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub(r\"www.\\S+\",'', x))\n",
    "#removing punctuation\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n",
    "#removing empty spaces and special characters\n",
    "df1['text']=df1['text'].apply(lambda x: re.sub('[^A-Za-z0-9]+',' ',x))\n",
    "#stopwords\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "# Print the set of spaCy's default stop words (remember that sets are unordered):\n",
    "print(nlp.Defaults.stop_words)\n",
    "#add twitter as a stop word and rt\n",
    "# Add the word to the set of stop words. Use LOWERCASE!\n",
    "nlp.Defaults.stop_words.add('twitter')\n",
    "nlp.Defaults.stop_words.add('rt')\n",
    "nlp.Defaults.stop_words.add('s')\n",
    "nlp.Defaults.stop_words.add('tweet')\n",
    "nlp.Defaults.stop_words.add('trend')\n",
    "nlp.Defaults.stop_words.add('follower')\n",
    "nlp.Defaults.stop_words.add('null')\n",
    "nlp.Defaults.stop_words.add('http')\n",
    "nlp.Defaults.stop_words.add('url')\n",
    "nlp.Defaults.stop_words.add('trend')\n",
    "nlp.Defaults.stop_words.add('trending')\n",
    "# Set the stop_word tag on the lexeme\n",
    "nlp.vocab['twitter'].is_stop = True\n",
    "nlp.vocab['rt'].is_stop = True\n",
    "nlp.vocab['s'].is_stop = True\n",
    "nlp.vocab['tweet'].is_stop = True\n",
    "nlp.vocab['trend'].is_stop = True\n",
    "nlp.vocab['follower'].is_stop = True\n",
    "nlp.vocab['null'].is_stop = True\n",
    "nlp.vocab['http'].is_stop = True\n",
    "nlp.vocab['url'].is_stop = True\n",
    "nlp.vocab['trend'].is_stop = True\n",
    "nlp.vocab['trending'].is_stop = True\n",
    "stop = nlp.Defaults.stop_words\n",
    "#lemmatisation\n",
    "# Loading model\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    " \n",
    "# Lemmatization with stopwords removal\n",
    "#df1[\"text\"]=df1[\"text\"].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if ((token.is_stop==False)&((token.pos_==\"ADJ\")|(token.pos_==\"NOUN\")|(token.pos_==\"VERB\")))]))\n",
    "#df1.head()\n",
    " \n",
    "df1[\"text\"]=df1[\"text\"].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if ((token.is_stop==False))]))\n",
    " \n",
    "#getting rid of empty text rows\n",
    "df1 = df1.dropna(subset=['text'])\n",
    " \n",
    "data = df1\n",
    " \n",
    "del df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VaderSentiment sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    " \n",
    "#VADER SENTIMENT\n",
    "#sentiment column\n",
    "# import SentimentIntensityAnalyzer class\n",
    "# from vaderSentiment.vaderSentiment module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid_obj = SentimentIntensityAnalyzer()\n",
    "#pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a new column\n",
    "lists = list(range(len(data)))\n",
    "data[\"Sentiment\"] = lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aply function to get sentiment for each row\n",
    "#df1['Sentiment'] = df1['text'].apply(lambda x: sid_obj.polarity_scores(x)['compound'])\n",
    "result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = pd.DataFrame(data[\"text\"] )\n",
    "type(data[\"text\"] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "     text = row['text']\n",
    "     sentiment  = sid_obj.polarity_scores(text)['compound']\n",
    "     result.append((sentiment))\n",
    " \n",
    "data['VaderSentiment'] = result\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.loc[1,\"created_at\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create VaderSentiment dataset to be used in time series comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacopy = data.copy()\n",
    "datacopy['month'] = datacopy['created_at'].dt.to_period('M')\n",
    "monthly_sentiment = datacopy.groupby('month')['VaderSentiment'].mean()\n",
    "monthly_sentiment = pd.DataFrame(monthly_sentiment)\n",
    "monthly_sentiment.to_csv(\"C:\\\\Users\\\\NB332021\\\\OneDrive - Nedbank\\\\Desktop\\\\Thesis\\\\Data\\\\monthlyvsent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the monthly sentiment\n",
    "import matplotlib.pyplot as plt\n",
    "monthly_sentiment.plot(kind='line', figsize=(10,6), marker='o', linestyle='-', color='blue')\n",
    "plt.title('Monthly Sentiment')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Sentiment')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#Vaderssentiment is between -1 and 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
